{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JakobPorsfelt/Content-Analysis-Assignments/blob/main/NLP_Exam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Classifying Sarcasm\n",
        "####This notebook works with a dataset on sarcasm from Kaggle: https://www.kaggle.com/datasets/danofer/sarcasm - It was originally made by Mikhail Khodak et. al. 2017 for their paper \"A Large Self-Annotated Corpus for Sarcasm\" - Has since been made public available on Kaggle. The dataset is self-annotated, meaning it is the authors who labeled their statement as sarcastic or not.\n",
        "\n",
        "The overall topic is sarcasm - The setting is a research into sarcasm - The main problem being how sarcasm can affect analysis of big text dataset in consumer research. This stury seek to understand sarcasm in order to be better at controlling for it, or understanding its context within sentiment analysis, in consumer behavior studies. Research has indicated there might be a relationship between sarcasm and polarity score (Agrawal & Papagelis 2020)(D. Tayal, S. et. al. 2014) therefore this relationship will be analyzed using VADER.\n",
        "This notebook seeks to both predict sarcasm through training different types of classifiers(Naive Bayes and CNN) as well as exploratively do a sentiment analysis to investigate whether there is a connection between sentiment and sarcasm.\n",
        "The main goal of the notebook is train to a binary classifier using different approaches and compare the results.\n",
        "The main problem is a big dataset and low computing power offered by colab free. However it is interesting to notice the performance of Naive Bayes vs CNN - Where CNN is heavy on resources but more complex, Naive Bayes is more simple but puts low demand on RAM and thus can train on more data and faster."
      ],
      "metadata": {
        "id": "_f9LvhQ6biKa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Naive Bayes Classifier"
      ],
      "metadata": {
        "id": "fIsNpbCseJGZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9dzPbcsYlDh"
      },
      "outputs": [],
      "source": [
        "!pip install gdown==4.6"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#downloading the dataset\n",
        "import gdown\n",
        "\n",
        "url='https://drive.google.com/uc?id=10LhzuH8143lOXGk6-e8-fDkXpu-xwXiH&confirm=t'\n",
        "output='train-balanced-sarcasm.csv'\n",
        "gdown.download(url, output, quiet=True, fuzzy=True, use_cookies=True)"
      ],
      "metadata": {
        "id": "C9vbyMlcTdxD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4ca6cd55-37bd-4729-cdb7-9f525bb89e2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'train-balanced-sarcasm.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYe9DeiqYlDj"
      },
      "outputs": [],
      "source": [
        "#Reading file and assigning to df object\n",
        "import pandas as pd\n",
        "file_path = '/content/train-balanced-sarcasm.csv'\n",
        "df = pd.read_csv(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jYMTHNFOtEb"
      },
      "outputs": [],
      "source": [
        "#Importing necessary modules\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "nltk.download('wordnet')\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtcIncWgbWVo"
      },
      "outputs": [],
      "source": [
        "#Checking for null/NA values\n",
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vekojsm-bWdm"
      },
      "outputs": [],
      "source": [
        "#droppping NA\n",
        "df = df.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Assigning data for train/test to variables for readability.\n",
        "comments = df['comment']\n",
        "labels = df['label']"
      ],
      "metadata": {
        "id": "YUn2nsSqbMpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4jOYevvMw3T"
      },
      "outputs": [],
      "source": [
        "#From the book with a minor tweak to text_clean because it was using deprecated methods, giving too many warning when running the script.\n",
        "#The script defines the different functions for normalizing the text data\n",
        "#- removing stop words, whitespaces, lowercase and only alphanumeric as well as lemmatizing etc.\n",
        "#Reference: Kedia, A., & Rasu, M. (2020).\n",
        "\n",
        "def text_clean(corpus):\n",
        "    cleaned_corpus_list = []\n",
        "    for row in corpus:\n",
        "        qs = []\n",
        "        for word in row.split():\n",
        "            p1 = re.sub(pattern='[^a-zA-Z0-9]', repl=' ', string=word)\n",
        "            p1 = p1.lower()\n",
        "            qs.append(p1)\n",
        "        cleaned_corpus_list.append(' '.join(qs))\n",
        "\n",
        "    cleaned_corpus = pd.Series(cleaned_corpus_list, dtype='object')\n",
        "    return cleaned_corpus\n",
        "\n",
        "def stopwords_removal(corpus):\n",
        "    stop = set(stopwords.words('english'))\n",
        "    corpus = [[x for x in x.split() if x not in stop] for x in corpus]\n",
        "    return corpus\n",
        "\n",
        "def lemmatize(corpus):\n",
        "    lem = WordNetLemmatizer()\n",
        "    corpus = [[lem.lemmatize(x, pos = 'v') for x in x] for x in corpus]\n",
        "    return corpus\n",
        "\n",
        "def stem(corpus, stem_type = None):\n",
        "    if stem_type == 'snowball':\n",
        "        stemmer = SnowballStemmer(language = 'english')\n",
        "        corpus = [[stemmer.stem(x) for x in x] for x in corpus]\n",
        "    else :\n",
        "        stemmer = PorterStemmer()\n",
        "        corpus = [[stemmer.stem(x) for x in x] for x in corpus]\n",
        "    return corpus\n",
        "def preprocess(corpus, cleaning = True, stemming = False, stem_type = None, lemmatization = False, remove_stopwords = True):\n",
        "\n",
        "    if cleaning == True:\n",
        "        corpus = text_clean(corpus)\n",
        "\n",
        "    if remove_stopwords == True:\n",
        "        corpus = stopwords_removal(corpus)\n",
        "    else :\n",
        "        corpus = [[x for x in x.split()] for x in corpus]\n",
        "\n",
        "    if lemmatization == True:\n",
        "        corpus = lemmatize(corpus)\n",
        "\n",
        "\n",
        "    if stemming == True:\n",
        "        corpus = stem(corpus, stem_type)\n",
        "\n",
        "    corpus = [' '.join(x) for x in corpus]\n",
        "\n",
        "\n",
        "    return corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FT6ZP42hMw8H"
      },
      "outputs": [],
      "source": [
        "#applying the function. Relevant to this is cleaning, lemmatization, and removing stopwords.\n",
        "comments_cleaned = preprocess(comments, cleaning = True, lemmatization = True, remove_stopwords = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOHLmdwfbLiU"
      },
      "outputs": [],
      "source": [
        "#vectorizing the data into tf-idf vectors. Essentially turning the words into weighted values.\n",
        "#Also assinging ngrams as it helps to improvs the accuracy of the model.\n",
        "tfidf = TfidfVectorizer(max_features=10000, ngram_range=(1, 2))\n",
        "X = tfidf.fit_transform(comments_cleaned)\n",
        "y = labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S68b1pUPbLlm"
      },
      "outputs": [],
      "source": [
        "#splitting the data into train and test parts\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2MCOVqOem-e"
      },
      "outputs": [],
      "source": [
        "#Training the model\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ScbMTSNnenFS"
      },
      "outputs": [],
      "source": [
        "#measuring the accuracy of the model.\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(4, 2))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the model is not doing a perfect job and your f1 score will probably be between 64-65%.\n",
        "The accuracay is a bit higher and sometimes hit almost 70% however the tradeoff considering the balanced distribution of the dataset, this is positive.\n",
        "Also the high precision shows that the model is able to predict sarcasm 2/3 of the time. Since predicting sarcasm and only sarcasm has an interest to this study, the recall is of lesser importance here. Overall the model is useful for doing explorative research for an overall interpertive result, where the importance of whether every single instance is correctly classified is less important."
      ],
      "metadata": {
        "id": "T-zJ0SVee7-7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Building CNN for sarcasm detection\n",
        "##### - Please switch to TPU runtime - type\n",
        "\n",
        "Now we will build a CNN to see if a neural network would do better than naive bayes. The CNN is more complex than the Naive Bayes allowing for taking into account positional variantions of tokens. We will be using google's pretrained word2vec model to embed the text data, allowing for representing semantic meaning of words as distance, and use this embedding as input for the neural network."
      ],
      "metadata": {
        "id": "uauFK-hpk7Dz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlVqH_6aC7FA"
      },
      "outputs": [],
      "source": [
        "!pip install gdown==4.6"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the file from gdrive\n",
        "import gdown\n",
        "\n",
        "url='https://drive.google.com/uc?id=10LhzuH8143lOXGk6-e8-fDkXpu-xwXiH&confirm=t'\n",
        "output='train-balanced-sarcasm.csv'\n",
        "gdown.download(url, output, quiet=True, fuzzy=True, use_cookies=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "quPWOIcXDgyM",
        "outputId": "128327ea-3fa0-4784-a0a2-683119fb3211"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'train-balanced-sarcasm.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = '/content/train-balanced-sarcasm.csv'\n",
        "df = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "A7GYEFlSDu4T"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import json\n",
        "import gensim\n",
        "import math\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from gensim.models import KeyedVectors\n",
        "import keras\n",
        "from keras.models import Sequential, Model\n",
        "from keras import layers\n",
        "from keras.layers import Dense, Dropout, Conv1D, GlobalMaxPooling1D\n",
        "import h5py\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "lXH74bE8Dg3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37b9b65e-f9b1-4566-b0d6-093f5094a78e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking for null values that would otherwise give errors when cleaning etc.\n",
        "df.isnull().values.sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dndCA9RBDnna",
        "outputId": "9b166a9d-40a9-41ed-b0a3-f4db75975a42"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "53"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing NANs\n",
        "df = df.dropna()"
      ],
      "metadata": {
        "id": "gfnQQwX6Dg1G"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Unfortanatly due to the low compute offered by colab free a sample is neeeded due to the large size of the dataset. CNN are RAM heavy\n",
        "#- Thus I am not able to push the whole dataset through the network like I was able to with the Naive Bayes Classifier.\n",
        "df_sample = df.sample(frac = 0.05, random_state = 42)\n",
        "\n",
        "#giving comments and labels their own variables - easier to read and work with.\n",
        "comments = df_sample['comment']\n",
        "labels = df_sample['label']"
      ],
      "metadata": {
        "id": "Xwhg7FO1yscH"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#How is the distribution of sacastic vs non-sarcastic? Ideally it should be an even split.\n",
        "#The distribution seems to be even.\n",
        "label_distribution = labels.value_counts()\n",
        "\n",
        "label_distribution.plot(kind='bar')\n",
        "plt.title('Label Distribution')\n",
        "plt.xlabel('Label')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "koNXO_oAki3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comments"
      ],
      "metadata": {
        "id": "Cp3d6i8ifNzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For good measure checking that the length are actually the same.\n",
        "len(comments) == len(labels)"
      ],
      "metadata": {
        "id": "TnprSjs7i2Oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#From the book with a minor tweak to text_clean because it was using deprecated methods, giving too many warning when running the script.\n",
        "#Reference: Kedia, A., & Rasu, M. (2020)\n",
        "\n",
        "\n",
        "def text_clean(corpus):\n",
        "    cleaned_corpus_list = []\n",
        "    for row in corpus:\n",
        "        qs = []\n",
        "        for word in row.split():\n",
        "            p1 = re.sub(pattern='[^a-zA-Z0-9]', repl=' ', string=word)\n",
        "            p1 = p1.lower()\n",
        "            qs.append(p1)\n",
        "        cleaned_corpus_list.append(' '.join(qs))\n",
        "\n",
        "    cleaned_corpus = pd.Series(cleaned_corpus_list, dtype='object')\n",
        "    return cleaned_corpus\n",
        "\n",
        "def stopwords_removal(corpus):\n",
        "    stop = set(stopwords.words('english'))\n",
        "    corpus = [[x for x in x.split() if x not in stop] for x in corpus]\n",
        "    return corpus\n",
        "\n",
        "def lemmatize(corpus):\n",
        "    lem = WordNetLemmatizer()\n",
        "    corpus = [[lem.lemmatize(x, pos = 'v') for x in x] for x in corpus]\n",
        "    return corpus\n",
        "\n",
        "def stem(corpus, stem_type = None):\n",
        "    if stem_type == 'snowball':\n",
        "        stemmer = SnowballStemmer(language = 'english')\n",
        "        corpus = [[stemmer.stem(x) for x in x] for x in corpus]\n",
        "    else :\n",
        "        stemmer = PorterStemmer()\n",
        "        corpus = [[stemmer.stem(x) for x in x] for x in corpus]\n",
        "    return corpus\n",
        "def preprocess(corpus, cleaning = True, stemming = False, stem_type = None, lemmatization = False, remove_stopwords = True):\n",
        "\n",
        "    if cleaning == True:\n",
        "        corpus = text_clean(corpus)\n",
        "\n",
        "    if remove_stopwords == True:\n",
        "        corpus = stopwords_removal(corpus)\n",
        "    else :\n",
        "        corpus = [[x for x in x.split()] for x in corpus]\n",
        "\n",
        "    if lemmatization == True:\n",
        "        corpus = lemmatize(corpus)\n",
        "\n",
        "\n",
        "    if stemming == True:\n",
        "        corpus = stem(corpus, stem_type)\n",
        "\n",
        "    corpus = [' '.join(x) for x in corpus]\n",
        "\n",
        "\n",
        "    return corpus"
      ],
      "metadata": {
        "id": "Xy5qR2ueEEYP"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Applying cleaning function\n",
        "comments_cleaned = preprocess(comments, cleaning = True, lemmatization = True, remove_stopwords = True)"
      ],
      "metadata": {
        "id": "6NfdibW0EEac"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "wKO-g_-WyXyI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf50a851-470c-4e57-a0af-972c2dd4910d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1OwbLwg-UlRvc9q1RPcCI6Oo64R-nA5wd\n",
            "To: /content/GoogleNews-vectors-negative300.bin.gz\n",
            "100% 1.65G/1.65G [00:28<00:00, 57.4MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown --fuzzy https://drive.google.com/uc?id=1OwbLwg-UlRvc9q1RPcCI6Oo64R-nA5wd\n",
        "!gzip -d /content/GoogleNews-vectors-negative300.bin.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "N1_CTD2vCSK2"
      },
      "outputs": [],
      "source": [
        "#building the word2vec model for embedding\n",
        "from gensim.models import KeyedVectors\n",
        "word2vec_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Measuring the length of the comments to calculate the mean length,\n",
        "#in order to get an understanding of a best estimate for max length for the cnn model\n",
        "\n",
        "df['comment_length'] = df['comment'].apply(lambda x: len(x.split()))\n",
        "df['comment_length'].mean()"
      ],
      "metadata": {
        "id": "R525RldsDg56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3642c461-72a9-41ba-db3a-2cc325fd8b8b"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10.461448811948875"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#max length should be about 10 since that is the average length of the sentences.\n",
        "#vectorsize is 300 since that is dimensionality of the pretrained word2vec model from google.\n",
        "MAX_LENGTH = 10\n",
        "VECTOR_SIZE = 300\n",
        "\n",
        "#Parameters for the model inspired by the book. Conservative numbers are unfortunately necessary due to low computing resources availabe.\n",
        "#around 30 epochs  seems to be most optimal in terms of loss. After 30 it doesnt improve much.\n",
        "#Reference: Kedia, A., & Rasu, M. (2020)\n",
        "\n",
        "FILTERS=8\n",
        "KERNEL_SIZE=3\n",
        "HIDDEN_LAYER_1_NODES=10\n",
        "HIDDEN_LAYER_2_NODES=5\n",
        "DROPOUT_PROB=0.35\n",
        "NUM_EPOCHS=30\n",
        "BATCH_SIZE=50"
      ],
      "metadata": {
        "id": "MS5y_WuvUkGa"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model using common activiation function, sigmoid typically used for binary classification.\n",
        "#Globalmaxpooling1D because the model handles text which is 1 dimensional.\n",
        "#Reference: Kedia, A., & Rasu, M. (2020)\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv1D(FILTERS,\n",
        "                 KERNEL_SIZE,\n",
        "                 padding='same',\n",
        "                 strides=1,\n",
        "                 activation='relu',\n",
        "                 input_shape = (MAX_LENGTH, VECTOR_SIZE)))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(HIDDEN_LAYER_1_NODES, activation='relu'))\n",
        "model.add(Dropout(DROPOUT_PROB))\n",
        "model.add(Dense(HIDDEN_LAYER_2_NODES, activation='relu'))\n",
        "model.add(Dropout(DROPOUT_PROB))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "P-dazT2TXFGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#compiling the model specifying the loss function etc.\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Qt27wcAc9_gn"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function for vectorizing the comment data\n",
        "#Reference: Kedia, A., & Rasu, M. (2020)\n",
        "\n",
        "def vectorize_data(data):\n",
        "\n",
        "    vectors = []\n",
        "\n",
        "    padding_vector = [0.0] * VECTOR_SIZE\n",
        "\n",
        "    for i, data_point in enumerate(data):\n",
        "        data_point_vectors = []\n",
        "        count = 0\n",
        "\n",
        "        tokens = data_point.split()\n",
        "\n",
        "        for token in tokens:\n",
        "            if count >= MAX_LENGTH:\n",
        "                break\n",
        "            if token in word2vec_model.key_to_index:\n",
        "                data_point_vectors.append(word2vec_model[token])\n",
        "            count = count + 1\n",
        "\n",
        "        if len(data_point_vectors) < MAX_LENGTH:\n",
        "            to_fill = MAX_LENGTH - len(data_point_vectors)\n",
        "            for _ in range(to_fill):\n",
        "                data_point_vectors.append(padding_vector)\n",
        "\n",
        "        vectors.append(data_point_vectors)\n",
        "\n",
        "    return vectors"
      ],
      "metadata": {
        "id": "PSTDywqiDhAe"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#applying the function to the comments and vectorizing them. Here embedding them with the word2vec model.\n",
        "vectorized_comments = vectorize_data(comments_cleaned)"
      ],
      "metadata": {
        "id": "6Mlve8tW2SJQ"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking if the comments were vectorized correctly, not going over the max length defined earlier.\n",
        "#Not returning anything means it is vectorized corrctly.\n",
        "for i, vec in enumerate(vectorized_comments):\n",
        "    if len(vec) != MAX_LENGTH:\n",
        "        print(i)"
      ],
      "metadata": {
        "id": "ziwwGZm1RrsH"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reshaping the data for inputs\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = np.reshape(vectorized_comments, (len(vectorized_comments), MAX_LENGTH, VECTOR_SIZE))\n",
        "y = np.array(labels)\n",
        "\n",
        "#Splitting the sample into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "#Training the model on the train data, validating on the test data.\n",
        "model.fit(X_train, y_train, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "e6-ipjwC9O5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#simple measure of accuracy of the predictions by the model on the test data.\n",
        "#Result is not great compared to the Naive Bayes, but further training on the rest of the dataset would probably improve the model.\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
      ],
      "metadata": {
        "id": "x5U_ybiVXFPq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5686cbfa-9e3f-42fd-b56e-672bf5a5511a"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing Accuracy:  0.6112\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluation of the model.\n",
        "#Because the classification metrics can't handle a mix of binary and continuous outputs, they have to be converted first.\n",
        "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(4, 2))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1C6sVYkBlOeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The CNN does a worse job than the Naive Bayes and it is most likely due to the smaller training set than the naive bayes.\n",
        "Since the amount of false positives and false negatives are very high, I wouldnt recommend using this model in this state as it is very likely to do wrong classification."
      ],
      "metadata": {
        "id": "sUZAcJU673za"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training on new datasets with identical column naming. Using subset as an example."
      ],
      "metadata": {
        "id": "ItnCItkplyEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a seperate df object for finetuning the model. Sotring as a .csv file to also showcase working with new data.\n",
        "#However this is granted that it has the same structure as the initial dataset.\n",
        "\n",
        "new_df = df.sample(frac = 0.01, random_state = 1)\n",
        "new_df.to_csv('new_df.csv')"
      ],
      "metadata": {
        "id": "mF8sQlDWF5UZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple function to fit the model on new data and print the updated accuracy of the model.\n",
        "# Also does preprocessing and vectorization.\n",
        "\n",
        "def train_new_data(file_path):\n",
        "  df = pd.read_csv(file_path)\n",
        "  df = df.dropna()\n",
        "  cleaned = preprocess(df['comment'], lemmatization = True, remove_stopwords = True)\n",
        "  vec_data = vectorize_data(cleaned)\n",
        "  X = np.reshape(vec_data, (len(vec_data), MAX_LENGTH, VECTOR_SIZE))\n",
        "  y = np.array(df['label'])\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "  model.fit(X_train, y_train, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE)\n",
        "  loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "  print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
      ],
      "metadata": {
        "id": "ZiFQ5C_cWUg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Applying the function\n",
        "train_new_data(\"new_df.csv\")"
      ],
      "metadata": {
        "id": "QPIdmIULYEPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Due to low computing resources the naive bayes probably did better than the cnn network since it could train on the whole dataset at once(1 million rows vs 50 thousand)\n",
        "####With the given resources the naive bayes is the better choice."
      ],
      "metadata": {
        "id": "pSP8LA7SBXxo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sentiment Analysis using VADER\n",
        "\n",
        "Here I analyse the sentiments of a sample from the dataset on sarcasm used above. I deploy VADER(Valence Aware Dictionary and sEntiment Reasoner) a lexicon-based sentiment analyzer - it returns the probablities for a given text being either positive, negative or neutral a long with a compound score. I will in this analysis use this compound score to define polarities as positive, negative or neutral. Papers indicate  that there is a connection between Polairty score and Sarcasm (Agrawal & Papagelis 2020)(D. Tayal, S. et. al. 2014)\n",
        "so this analysis is about investigating the distribution of sentiments and to investigate(At a glance) if this can say anything about this relationship."
      ],
      "metadata": {
        "id": "BIQGj-AoEqOx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5-_--Bwpoy2"
      },
      "outputs": [],
      "source": [
        "!pip install gdown==4.6"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing data\n",
        "import gdown\n",
        "\n",
        "url='https://drive.google.com/uc?id=10LhzuH8143lOXGk6-e8-fDkXpu-xwXiH&confirm=t'\n",
        "output='train-balanced-sarcasm.csv'\n",
        "gdown.download(url, output, quiet=True, fuzzy=True, use_cookies=True)\n",
        "\n",
        "import pandas as pd\n",
        "file_path = '/content/train-balanced-sarcasm.csv'\n",
        "df = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "fCvh2f4wpoy-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Drop NA values\n",
        "df = df.dropna()\n",
        "\n",
        "#Due to time and resources I take a sample of about 100k comments.\n",
        "df_sample = df.sample(frac = 0.1, random_state = 42)\n",
        "\n",
        "#creating and deploying the sentiment analyzer on the comment column.\n",
        "import nltk\n",
        "\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def analyze_sentiment(comment):\n",
        "    return analyzer.polarity_scores(comment)\n",
        "\n",
        "df_sample['sentiment_score'] = df_sample['comment'].apply(analyze_sentiment)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pw1j95Fvk7RO",
        "outputId": "4693ff59-8276-4af4-b9f5-5dd82305ea3c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sentiment score is made up of probalistic values that the comment is either positive, neutral or negative, it also gives a compound value which we will use to define definitions of polarity.\n",
        "df_sample[['comment','sentiment_score']]"
      ],
      "metadata": {
        "id": "7N8URlMhmMTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a polarity score definition translating into either postive (value > 0.05) negative (value < -0.05) or otherwise neutral.\n",
        "def sentiment(score):\n",
        "    comp = 0\n",
        "    sentiment = \"\"\n",
        "    for key, val in score.items():\n",
        "        if key == \"compound\":\n",
        "          if val >= 0.05:\n",
        "              sentiment = \"Positive\"\n",
        "          elif val <= -0.05:\n",
        "              sentiment = \"Negative\"\n",
        "          else:\n",
        "              sentiment = \"Neutral\"\n",
        "    return sentiment"
      ],
      "metadata": {
        "id": "fz39H1LjpG-4"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#applying the function to the scores, extracting and translating the value into a \"Predominant sentiment\"\n",
        "df_sample['Predominant_Polarity'] = df_sample['sentiment_score'].apply(sentiment)\n",
        "\n",
        "#Checking the distribution of predominant sentiment\n",
        "total = df_sample['Predominant_Polarity'].value_counts()"
      ],
      "metadata": {
        "id": "5-VTDrgWvP5s"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total"
      ],
      "metadata": {
        "id": "c_soM7fZFjBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neutral being the biggest category, however postive/negative being bigger.\n",
        "This could indicate a polarity - The dataset is obviously about sarcasm and sarcasm is a kind of irony where one says something with an opposite meaning."
      ],
      "metadata": {
        "id": "mgXij9NRFlA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Cheking the distribution of label and predominant_sentiment.\n",
        "distribution = df_sample.groupby(['Predominant_Polarity', 'label']).size().reset_index(name='counts')\n",
        "print(distribution)"
      ],
      "metadata": {
        "id": "st3d9kMrvZRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a simple bar-plot\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "distribution = df_sample.groupby(['Predominant_Polarity', 'label']).size().reset_index(name='counts')\n",
        "\n",
        "sns.barplot(x='Predominant_Polarity', y='counts', hue='label', data=distribution)\n",
        "\n",
        "plt.xlabel('Predominant Polarity')\n",
        "plt.ylabel('Counts')\n",
        "plt.title('Distribution of Labels by Predominant Polarity')\n",
        "plt.legend(title='label')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "df_sample['compound'] = df_sample['sentiment_score'].apply(lambda x: x['compound'])\n",
        "\n",
        "from scipy.stats import pointbiserial\n",
        "\n",
        "correlation, p_value = pointbiserialr(df_sample['label'], df_sample['compound'])\n",
        "print(\"Correlation coefficient:\", correlation)\n",
        "print(\"P-value:\", p_value)\n",
        "\n"
      ],
      "metadata": {
        "id": "VJ_HOGv7yDOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It does not seem that any obvious relationships exist in this context. The dataset is more negative or positive than neutral, but this could either be due to how the dataset was created or be due to the dataset revolving around sarcasm which obviously states something but means something different. Then whether positive is truly positive as per opinion or negative is hard to tell - Could sarcasm mean that positives are equal to a negative sentiment and vice versa and how often is this the case?\n",
        "\n",
        "It is therefore a quite complex issue and would require additional analysis of the semantics as well as complex topic modelling(perhaps with the aid of LLM)\n",
        "\n",
        "Also analyzing the correlation in terms of P-value using point-biserial correlation, indicate that there isn't a clear relationship between the two in this context."
      ],
      "metadata": {
        "id": "HyhqUoBuNFsO"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP/6I7jRPEOloiHApwyCEip",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}